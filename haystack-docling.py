import os
import shutil
import pickle
from typing import List, Dict, Any

import gradio as gr
from dotenv import load_dotenv

# Docling core
from docling.document_converter import DocumentConverter
from docling_core.transforms.chunker import HierarchicalChunker

# OpenAI client
from openai import OpenAI
import numpy as np

# Load API key
load_dotenv()

# Th∆∞ m·ª•c l∆∞u file v√† index
DOCUMENTS_DIR = "documents"
INDEX_PATH = "vector_index.pkl"
os.makedirs(DOCUMENTS_DIR, exist_ok=True)

# OpenAI c·∫•u h√¨nh
_openai_client = OpenAI()
EMBED_MODEL = "text-embedding-3-small"
CHAT_MODEL = "gpt-4o-mini"


# ====== INDEX ======
def _load_index() -> List[Dict[str, Any]]:
    if os.path.exists(INDEX_PATH):
        with open(INDEX_PATH, "rb") as f:
            return pickle.load(f)
    return []


def _save_index(items: List[Dict[str, Any]]):
    with open(INDEX_PATH, "wb") as f:
        pickle.dump(items, f)


_index: List[Dict[str, Any]] = _load_index()


def _embed_texts(texts: List[str]) -> List[List[float]]:
    if not texts:
        return []
    res = _openai_client.embeddings.create(input=texts, model=EMBED_MODEL)
    return [d.embedding for d in res.data]


def _cosine_sim(a: np.ndarray, b: np.ndarray) -> float:
    na = np.linalg.norm(a)
    nb = np.linalg.norm(b)
    if na == 0 or nb == 0:
        return 0.0
    return float(np.dot(a, b) / (na * nb))


# ====== DOC CONVERT ======
def convert_with_docling(file_path: str) -> List[str]:
    """
    ƒê·ªçc file b·∫±ng Docling v√† chunk th√†nh danh s√°ch ƒëo·∫°n vƒÉn b·∫£n.
    """
    converter = DocumentConverter()
    chunker = HierarchicalChunker()
    result = converter.convert(file_path)
    docling_doc = result.document
    chunks = list(chunker.chunk(docling_doc))
    texts: List[str] = []
    for ch in chunks:
        content = ch.text or ""
        if content.strip():
            texts.append(content)
    return texts


# ====== QU·∫¢N L√ù FILE/INDEX ======
def upload_file(file):
    if file is None:
        return "Kh√¥ng c√≥ file n√†o ƒë∆∞·ª£c t·∫£i l√™n."

    filename = os.path.basename(file.name)
    ext = os.path.splitext(filename)[1].lower()
    if ext not in [".pdf", ".docx", ".txt", ".md"]:
        return f"ƒê·ªãnh d·∫°ng {ext} kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£."

    file_path = os.path.join(DOCUMENTS_DIR, filename)
    if os.path.exists(file_path):
        return f"File {filename} ƒë√£ t·ªìn t·∫°i."

    shutil.copy(file.name, file_path)

    # Convert -> embed -> ghi v√†o index n·ªôi b·ªô
    texts = convert_with_docling(file_path)
    if not texts:
        return f"Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c n·ªôi dung t·ª´ {filename}"

    embeddings = _embed_texts(texts)
    if not embeddings:
        return f"Kh√¥ng th·ªÉ t·∫°o embedding cho {filename}"

    for text, emb in zip(texts, embeddings):
        _index.append(
            {
                "content": text,
                "meta": {
                    "source_file": filename,
                },
                "embedding": emb,
            }
        )

    _save_index(_index)
    return f"File {filename} ƒë√£ ƒë∆∞·ª£c t·∫£i l√™n v√† ƒë√°nh index th√†nh c√¥ng."


def list_documents():
    return "\n".join(os.listdir(DOCUMENTS_DIR)) or "No documents."


def delete_document(filename):
    file_path = os.path.join(DOCUMENTS_DIR, filename)
    if not os.path.exists(file_path):
        return f"Kh√¥ng t√¨m th·∫•y file: {filename}"

    os.remove(file_path)

    # X√≥a kh·ªèi index n·ªôi b·ªô
    global _index
    _index = [it for it in _index if it.get("meta", {}).get("source_file") != filename]
    _save_index(_index)
    return f"File {filename} ƒë√£ ƒë∆∞·ª£c x√≥a."


# ====== H·ªéI ƒê√ÅP ======
def _build_prompt(query: str, items: List[Dict[str, Any]]) -> str:
    context_blocks = []
    for i, it in enumerate(items, start=1):
        content = it.get("content", "")
        meta = it.get("meta", {})
        source = meta.get("source_file", "?")
        cit = f"[{source}]"
        context_blocks.append(f"Ngu·ªìn {i} {cit}:\n{content}")

    context_text = "\n\n".join(context_blocks)
    prompt = (
        "B·∫°n l√† m·ªôt tr·ª£ l√Ω AI t√¨m ki·∫øm th√¥ng tin th√¥ng minh.\n"
        "S·ª≠ d·ª•ng ch√≠nh x√°c th√¥ng tin t·ª´ t√†i li·ªáu d∆∞·ªõi ƒë√¢y ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi.\n"
        'N·∫øu kh√¥ng bi·∫øt, h√£y tr·∫£ l·ªùi "T√¥i kh√¥ng bi·∫øt".\n'
        "B·∫Øt bu·ªôc ph·∫£i k√®m ngu·ªìn tr√≠ch d·∫´n theo ƒë·ªãnh d·∫°ng: [T√™n file, Trang X]\n\n"
        f"Ng·ªØ c·∫£nh:\n{context_text}\n\n"
        f"C√¢u h·ªèi: {query}\n\n"
        "Tr·∫£ l·ªùi:"
    )
    return prompt


def ask_question(question, num_results=5):
    if not str(question or "").strip():
        return "Vui l√≤ng nh·∫≠p c√¢u h·ªèi."

    if not _index:
        return "Ch∆∞a c√≥ t√†i li·ªáu n√†o trong h·ªá th·ªëng."

    # Embed c√¢u h·ªèi
    q_emb = _embed_texts([question])
    if not q_emb:
        return "Kh√¥ng t·∫°o ƒë∆∞·ª£c embedding cho c√¢u h·ªèi."
    qv = np.array(q_emb[0], dtype=float)

    # T√≠nh cosine v·ªõi t·∫•t c·∫£ documents
    scores = []
    for it in _index:
        dv = np.array(it.get("embedding") or [], dtype=float)
        scores.append(_cosine_sim(qv, dv))

    # L·∫•y top-k
    k = max(1, int(num_results))
    top_idx = np.argsort(scores)[-k:][::-1]
    top_items = [_index[i] for i in top_idx]
    if not top_items:
        return "Kh√¥ng t√¨m th·∫•y th√¥ng tin trong t√†i li·ªáu."

    # G·ªçi OpenAI sinh c√¢u tr·∫£ l·ªùi
    prompt = _build_prompt(question, top_items)
    resp = _openai_client.chat.completions.create(
        model=CHAT_MODEL,
        messages=[
            {"role": "system", "content": "B·∫°n tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát."},
            {"role": "user", "content": prompt},
        ],
        temperature=0.2,
    )
    return (
        resp.choices[0].message.content
        if resp.choices
        else "Kh√¥ng t·∫°o ƒë∆∞·ª£c c√¢u tr·∫£ l·ªùi."
    )


# ========== GRADIO UI ==========
with gr.Blocks(title="RAG QA - Docling + OpenAI (Local Vector Store)") as demo:
    gr.Markdown("# ü§ñ RAG-based QA System (Docling + OpenAI)")

    with gr.Tab("üìÑ Qu·∫£n l√Ω T√†i li·ªáu"):
        with gr.Row():
            file_upload = gr.File(
                label="Upload", file_types=[".pdf", ".docx", ".txt", ".md"]
            )
            upload_btn = gr.Button("üì§ Upload & Index")
            upload_out = gr.Textbox(label="K·∫øt qu·∫£", lines=5)

        list_btn = gr.Button("üìã Li·ªát k√™")
        list_out = gr.Textbox(label="Danh s√°ch", lines=5)

        delete_in = gr.Textbox(label="T√™n file ƒë·ªÉ x√≥a")
        delete_btn = gr.Button("üóëÔ∏è X√≥a")
        delete_out = gr.Textbox(label="K·∫øt qu·∫£", lines=5)

        upload_btn.click(upload_file, inputs=file_upload, outputs=upload_out)
        list_btn.click(list_documents, outputs=list_out)
        delete_btn.click(delete_document, inputs=delete_in, outputs=delete_out)

    with gr.Tab("üí¨ H·ªèi ƒë√°p"):
        chatbot = gr.Chatbot(height=400)
        question_in = gr.Textbox(label="C√¢u h·ªèi")
        submit_btn = gr.Button("üöÄ G·ª≠i")
        num_results = gr.Slider(1, 10, value=5, step=1, label="S·ªë k·∫øt qu·∫£")

        def user(message, history):
            return "", history + [[message, None]]

        def bot(history, num_results):
            q = history[-1][0]
            answer = ask_question(q, num_results)
            history[-1][1] = answer
            return history

        question_in.submit(user, [question_in, chatbot], [question_in, chatbot]).then(
            bot, [chatbot, num_results], chatbot
        )
        submit_btn.click(user, [question_in, chatbot], [question_in, chatbot]).then(
            bot, [chatbot, num_results], chatbot
        )


demo.launch(server_name="localhost", server_port=7860)
