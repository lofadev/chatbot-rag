import os
import shutil
import gradio as gr
from langchain.document_loaders import PyMuPDFLoader, Docx2txtLoader, TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
from dotenv import load_dotenv

# T·∫£i c√°c bi·∫øn m√¥i tr∆∞·ªùng (cho OpenAI API key)
load_dotenv()

# C√°c th∆∞ m·ª•c
DOCUMENTS_DIR = "documents"
INDEX_DIR = "faiss_index"

# ƒê·∫£m b·∫£o c√°c th∆∞ m·ª•c t·ªìn t·∫°i
os.makedirs(DOCUMENTS_DIR, exist_ok=True)
os.makedirs(INDEX_DIR, exist_ok=True)

# C·∫•u h√¨nh m·∫∑c ƒë·ªãnh cho chunking
DEFAULT_CONFIG = {
    "chunk_size": 1000,
    "chunk_overlap": 200,
    "strategy": "recursive",  # "recursive", "character", "token"
    "separators": ["\n\n", "\n", " ", ""],
    "length_function": "len",
}


# H√†m l·∫•y loader ph√π h·ª£p d·ª±a tr√™n ph·∫ßn m·ªü r·ªông c·ªßa file
def get_loader(file_path):
    ext = os.path.splitext(file_path)[1].lower()
    if ext == ".pdf":
        return PyMuPDFLoader(file_path, extract_tables="markdown", mode="page")
    elif ext == ".docx":
        return Docx2txtLoader(file_path)
    elif ext in [".txt", ".md"]:
        return TextLoader(file_path)
    else:
        raise ValueError("Unsupported file format")


# H√†m x·ª≠ l√Ω m·ªôt t√†i li·ªáu ƒë∆°n l·∫ª v·ªõi metadata chi ti·∫øt
def ingest_document(file_path, config=None):
    if config is None:
        config = DEFAULT_CONFIG.copy()

    filename = os.path.basename(file_path)
    loader = get_loader(file_path)
    documents = loader.load()

    # C·∫£i thi·ªán metadata cho m·ªói document
    for doc in documents:
        doc.metadata.update(
            {
                "source_file": filename,
                "file_path": file_path,
                "file_type": os.path.splitext(filename)[1].lower(),
                "processed_time": str(os.path.getmtime(file_path)),
            }
        )

        # Th√™m page number n·∫øu c√≥ (c·ªông th√™m 1 ƒë·ªÉ b·∫Øt ƒë·∫ßu t·ª´ trang 1)
        if "page" in doc.metadata:
            page_info = doc.metadata.get("page")
            if page_info is not None and isinstance(page_info, (int, float)):
                doc.metadata["page_number"] = int(page_info) + 1
            else:
                doc.metadata["page_number"] = page_info
        else:
            # Kh√¥ng c√≥ th√¥ng tin trang
            doc.metadata["page_number"] = "N/A"

    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=config["chunk_size"],
        chunk_overlap=config["chunk_overlap"],
        length_function=len,
        separators=config.get("separators", ["\n\n", "\n", " ", ""]),
    )
    chunks = text_splitter.split_documents(documents)

    # Th√™m th√¥ng tin chunk index cho m·ªói chunk
    for i, chunk in enumerate(chunks):
        chunk.metadata.update(
            {
                "chunk_index": i,
                "chunk_size": len(chunk.page_content),
                "total_chunks": len(chunks),
            }
        )

    return chunks


# H√†m x√¢y d·ª±ng ho·∫∑c x√¢y d·ª±ng l·∫°i index v·ªõi c·∫•u h√¨nh chunking
def build_index(config=None):
    if config is None:
        config = DEFAULT_CONFIG.copy()

    if os.path.exists(INDEX_DIR):
        shutil.rmtree(INDEX_DIR)

    all_chunks = []
    processed_files = []

    for filename in os.listdir(DOCUMENTS_DIR):
        if filename.startswith("."):  # B·ªè qua hidden files
            continue

        file_path = os.path.join(DOCUMENTS_DIR, filename)
        if os.path.isfile(file_path):
            try:
                chunks = ingest_document(file_path, config)
                all_chunks.extend(chunks)
                processed_files.append(f"{filename}: {len(chunks)} chunks")
            except Exception as e:
                processed_files.append(f"{filename}: Error - {str(e)}")

    if all_chunks:
        embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
        vectorstore = FAISS.from_documents(all_chunks, embeddings)
        vectorstore.save_local(INDEX_DIR)

        result = f"Index built successfully with {len(all_chunks)} total chunks.\n"
        result += f"Chunking strategy: {config['strategy']}, "
        result += f"Chunk size: {config['chunk_size']}, "
        result += f"Overlap: {config['chunk_overlap']}\n\n"
        result += "Processed files:\n" + "\n".join(processed_files)
        return result
    else:
        return "No documents to index."


# H√†m t·∫£i vectorstore
def load_vectorstore():
    if os.path.exists(os.path.join(INDEX_DIR, "index.faiss")):
        embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
        return FAISS.load_local(
            INDEX_DIR, embeddings, allow_dangerous_deserialization=True
        )
    return None


# H√†m t·∫£i l√™n v√† x·ª≠ l√Ω file
def upload_file(file):
    if file is None:
        return "No file uploaded."

    filename = os.path.basename(file.name)
    file_path = os.path.join(DOCUMENTS_DIR, filename)
    shutil.copy(file.name, file_path)

    config = DEFAULT_CONFIG.copy()
    result = build_index(config)  # Rebuild index sau khi t·∫£i l√™n
    return f"File {filename} uploaded successfully.\n\n{result}"


# H√†m li·ªát k√™ c√°c t√†i li·ªáu v·ªõi th√¥ng tin chi ti·∫øt
def list_documents():
    files = os.listdir(DOCUMENTS_DIR)
    if not files:
        return "No documents."

    file_info = []
    for filename in files:
        if filename.startswith("."):
            continue
        file_path = os.path.join(DOCUMENTS_DIR, filename)
        if os.path.isfile(file_path):
            size = os.path.getsize(file_path)
            size_mb = round(size / (1024 * 1024), 2)
            ext = os.path.splitext(filename)[1].lower()
            file_info.append(f"üìÑ {filename} ({size_mb} MB, {ext})")

    return "\n".join(file_info) if file_info else "No valid documents."


# H√†m x√≥a t√†i li·ªáu
def delete_document(filename):
    file_path = os.path.join(DOCUMENTS_DIR, filename)
    if os.path.exists(file_path):
        os.remove(file_path)
        config = DEFAULT_CONFIG.copy()
        result = build_index(config)  # Rebuild index sau khi x√≥a
        return f"File {filename} deleted successfully.\n\n{result}"
    return "File not found."


# H√†m ƒë√°nh index l·∫°i v·ªõi c·∫•u h√¨nh hi·ªán t·∫°i
def reindex():
    config = DEFAULT_CONFIG.copy()
    return build_index(config)


# Prompt t√πy ch·ªânh cho LLM v·ªõi h∆∞·ªõng d·∫´n tr√≠ch d·∫´n chi ti·∫øt
prompt_template = """
B·∫°n l√† m·ªôt tr·ª£ l√Ω AI t√¨m ki·∫øm th√¥ng tin th√¥ng minh, b·∫°n c√≥ th·ªÉ t√¨m ki·∫øm th√¥ng tin trong c√°c t√†i li·ªáu ƒë√£ ƒë∆∞·ª£c ƒë√°nh index.
S·ª≠ d·ª•ng CH√çNH X√ÅC th√¥ng tin t·ª´ c√°c ƒëo·∫°n vƒÉn b·∫£n d∆∞·ªõi ƒë√¢y ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi. 
N·∫øu b·∫°n kh√¥ng bi·∫øt c√¢u tr·∫£ l·ªùi t·ª´ ng·ªØ c·∫£nh ƒë√£ cho, h√£y n√≥i r·∫±ng b·∫°n kh√¥ng bi·∫øt, ƒë·ª´ng b·ªãa ƒë·∫∑t th√¥ng tin.
S·ª≠ d·ª•ng ng√¥n ng·ªØ t·ª± nhi√™n, th√¢n thi·ªán v·ªõi ng∆∞·ªùi d√πng.
Tr·∫£ l·ªùi b·∫±ng c√πng ng√¥n ng·ªØ v·ªõi c√¢u h·ªèi (Ti·∫øng Vi·ªát ho·∫∑c Ti·∫øng Anh).
Khi c√¢u h·ªèi b·∫±ng ti·∫øng Anh nh∆∞ng context b·∫±ng ti·∫øng Vi·ªát, h√£y hi·ªÉu context ti·∫øng Vi·ªát, tr√≠ch xu·∫•t th√¥ng tin li√™n quan, v√† so·∫°n c√¢u tr·∫£ l·ªùi ho√†n ch·ªânh b·∫±ng ti·∫øng Anh. Ng∆∞·ª£c l·∫°i Ti·∫øng Anh c≈©ng v·∫≠y.

QUY T·∫ÆC TR√çCH D·∫™N B·∫ÆT BU·ªòC:
- B·∫ÆT BU·ªòC ph·∫£i tr√≠ch d·∫´n ngu·ªìn cho M·ªñI th√¥ng tin b·∫°n cung c·∫•p
- S·ª¨ D·ª§NG CH√çNH X√ÅC citation ƒë√£ ƒë∆∞·ª£c cung c·∫•p trong t·ª´ng ƒëo·∫°n vƒÉn b·∫£n
- M·ªói ƒëo·∫°n vƒÉn b·∫£n ƒë√£ c√≥ s·∫µn "Ngu·ªìn tr√≠ch d·∫´n:" ·ªü cu·ªëi - PH·∫¢I s·ª≠ d·ª•ng ƒë√∫ng citation n√†y
- KH√îNG ƒë∆∞·ª£c t·ª± t·∫°o citation m·ªõi, ch·ªâ s·ª≠ d·ª•ng citation c√≥ s·∫µn trong ng·ªØ c·∫£nh
- Khi tham kh·∫£o th√¥ng tin t·ª´ m·ªôt ƒëo·∫°n vƒÉn b·∫£n, LU√îN include citation c·ªßa ƒëo·∫°n ƒë√≥
- N·∫øu th√¥ng tin ƒë·∫øn t·ª´ nhi·ªÅu ƒëo·∫°n vƒÉn b·∫£n, li·ªát k√™ T·∫§T C·∫¢ citations li√™n quan

C√ÅCH TR√çCH D·∫™N:
- Sau m·ªói th√¥ng tin, th√™m citation trong ngo·∫∑c vu√¥ng
- V√≠ d·ª•: "Doanh thu nƒÉm 2023 l√† 100 tri·ªáu ƒë·ªìng [bao_cao_tai_chinh.pdf, Trang 5]"
- V·ªõi nhi·ªÅu ngu·ªìn: "Th√¥ng tin n√†y ƒë∆∞·ª£c x√°c nh·∫≠n [file1.pdf, Trang 2] [file2.pdf, Trang 7]"

Ng·ªØ c·∫£nh v·ªõi citation:
{context}

C√¢u h·ªèi: {question}

Tr·∫£ l·ªùi chi ti·∫øt (B·∫ÆT BU·ªòC bao g·ªìm tr√≠ch d·∫´n ngu·ªìn cho m·ªçi th√¥ng tin):
"""

PROMPT = PromptTemplate(
    template=prompt_template, input_variables=["context", "question"]
)


# H√†m c·∫£i thi·ªán context v·ªõi metadata chi ti·∫øt - ch·ªâ hi·ªÉn th·ªã file v√† trang
def format_docs_with_metadata(docs):
    formatted_docs = []
    for doc in docs:
        metadata = doc.metadata
        source_file = metadata.get("source_file", "Unknown file")

        # X·ª≠ l√Ω s·ªë trang (∆∞u ti√™n page_number ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω, kh√¥ng th√¨ x·ª≠ l√Ω page g·ªëc)
        if "page_number" in metadata and metadata["page_number"] != "N/A":
            # page_number ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω trong ingest_document
            display_page = metadata["page_number"]
            citation = f"[{source_file}, Trang {display_page}]"
        elif "page" in metadata and metadata["page"] != "N/A":
            # Fallback: s·ª≠ d·ª•ng page g·ªëc v√† c·ªông th√™m 1
            page_info = metadata["page"]
            if isinstance(page_info, (int, float)):
                display_page = int(page_info) + 1
            else:
                display_page = page_info
            citation = f"[{source_file}, Trang {display_page}]"
        else:
            # Kh√¥ng c√≥ th√¥ng tin trang, ch·ªâ hi·ªÉn th·ªã file
            citation = f"[{source_file}]"

        # Format content v·ªõi citation r√µ r√†ng h∆°n - kh√¥ng hi·ªÉn th·ªã th√¥ng tin chunk
        content = f"=== ƒêO·∫†N VƒÇN B·∫¢N T·ª™ {citation} ===\n{doc.page_content}\n=== K·∫æT TH√öC ƒêO·∫†N VƒÇN B·∫¢N ===\nNgu·ªìn tr√≠ch d·∫´n: {citation}"
        formatted_docs.append(content)

    return "\n\n---\n\n".join(formatted_docs)


# H√†m cho h·ªèi ƒë√°p v·ªõi citation ch√≠nh x√°c
def ask_question(question, num_results=5):
    vectorstore = load_vectorstore()
    if vectorstore is None:
        return "Kh√¥ng c√≥ index n√†o kh·∫£ d·ª•ng. Vui l√≤ng t·∫£i l√™n t√†i li·ªáu tr∆∞·ªõc."

    # T·∫°o LLM
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

    # T·∫°o retriever
    retriever = vectorstore.as_retriever(search_kwargs={"k": num_results})

    # Retrieve documents
    relevant_docs = retriever.get_relevant_documents(question)

    if not relevant_docs:
        return "Kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan trong t√†i li·ªáu."

    # Format context v·ªõi metadata chi ti·∫øt
    formatted_context = format_docs_with_metadata(relevant_docs)

    # T·∫°o prompt v·ªõi context ƒë√£ ƒë∆∞·ª£c format
    formatted_prompt = PROMPT.format(context=formatted_context, question=question)

    # G·ªçi LLM v·ªõi prompt ƒë√£ format
    result = llm.invoke(formatted_prompt)

    # Tr·∫£ v·ªÅ n·ªôi dung text t·ª´ response
    if hasattr(result, "content"):
        return result.content
    else:
        return str(result)


# Giao di·ªán Gradio
with gr.Blocks(title="RAG Document QA System", theme=gr.themes.Default()) as demo:
    gr.Markdown("# ü§ñ RAG-based Document QA System")
    gr.Markdown(
        "H·ªá th·ªëng h·ªèi ƒë√°p th√¥ng minh d·ª±a tr√™n t√†i li·ªáu v·ªõi kh·∫£ nƒÉng tr√≠ch d·∫´n ngu·ªìn chi ti·∫øt"
    )

    with gr.Tab("üìÑ Qu·∫£n l√Ω T√†i li·ªáu"):
        gr.Markdown("### T·∫£i l√™n v√† qu·∫£n l√Ω t√†i li·ªáu")

        with gr.Row():
            with gr.Column(scale=2):
                file_upload = gr.File(
                    label="Ch·ªçn t√†i li·ªáu (PDF, DOCX, TXT, MD)",
                    file_types=[".pdf", ".docx", ".txt", ".md"],
                )
                upload_button = gr.Button("üì§ T·∫£i l√™n v√† ƒê√°nh index", variant="primary")
                upload_output = gr.Textbox(label="Tr·∫°ng th√°i t·∫£i l√™n", lines=5)

            with gr.Column(scale=1):
                list_button = gr.Button("üìã Li·ªát k√™ t√†i li·ªáu")
                documents_list = gr.Textbox(label="Danh s√°ch t√†i li·ªáu", lines=8)

        gr.Markdown("---")

        with gr.Row():
            delete_input = gr.Textbox(
                label="T√™n file c·∫ßn x√≥a", placeholder="V√≠ d·ª•: document.pdf"
            )
            delete_button = gr.Button("üóëÔ∏è X√≥a t√†i li·ªáu", variant="stop")
            delete_output = gr.Textbox(label="Tr·∫°ng th√°i x√≥a", lines=3)

        with gr.Row():
            reindex_button = gr.Button("üîÑ ƒê√°nh index l·∫°i to√†n b·ªô", variant="secondary")
            reindex_output = gr.Textbox(label="Tr·∫°ng th√°i ƒë√°nh index", lines=5)

        # Event handlers cho tab Admin
        upload_button.click(upload_file, inputs=file_upload, outputs=upload_output)
        list_button.click(list_documents, outputs=documents_list)
        delete_button.click(delete_document, inputs=delete_input, outputs=delete_output)
        reindex_button.click(reindex, outputs=reindex_output)

    with gr.Tab("üí¨ H·ªèi ƒë√°p"):
        gr.Markdown("### ƒê·∫∑t c√¢u h·ªèi v·ªÅ t√†i li·ªáu")
        gr.Markdown(
            "H·ªá th·ªëng s·∫Ω t√¨m ki·∫øm th√¥ng tin li√™n quan v√† tr·∫£ l·ªùi k√®m tr√≠ch d·∫´n ngu·ªìn"
        )

        with gr.Row():
            with gr.Column(scale=3):
                chatbot = gr.Chatbot(
                    height=500, label="Cu·ªôc h·ªôi tho·∫°i", show_label=True
                )

                with gr.Row():
                    msg = gr.Textbox(
                        label="C√¢u h·ªèi c·ªßa b·∫°n",
                        placeholder="V√≠ d·ª•: T√†i li·ªáu n√≥i g√¨ v·ªÅ ch√≠nh s√°ch b·∫£o m·∫≠t?",
                        lines=2,
                        scale=4,
                    )
                    submit_btn = gr.Button("üöÄ G·ª≠i", variant="primary", scale=1)
                    clear_btn = gr.Button("üóëÔ∏è X√≥a", variant="secondary", scale=1)

            with gr.Column(scale=1):
                gr.Markdown("### ‚ö° T√πy ch·ªçn t√¨m ki·∫øm")
                num_results = gr.Slider(
                    minimum=1,
                    maximum=10,
                    value=5,
                    step=1,
                    label="S·ªë k·∫øt qu·∫£ t√¨m ki·∫øm",
                    info="Nhi·ªÅu k·∫øt qu·∫£ = ng·ªØ c·∫£nh phong ph√∫ h∆°n nh∆∞ng ch·∫≠m h∆°n",
                )

                gr.Markdown("### üìù G·ª£i √Ω s·ª≠ d·ª•ng")
                gr.Markdown(
                    """
                - ƒê·∫∑t c√¢u h·ªèi r√µ r√†ng v√† c·ª• th·ªÉ
                - S·ª≠ d·ª•ng t·ª´ kh√≥a c√≥ trong t√†i li·ªáu
                - C√¢u tr·∫£ l·ªùi s·∫Ω k√®m theo tr√≠ch d·∫´n ngu·ªìn (v√≠ d·ª•: [T√™n file, Trang X])
                """
                )

        def user(user_message, history):
            return "", history + [[user_message, None]]

        def bot(history, num_results):
            if history and history[-1][0]:
                bot_message = ask_question(history[-1][0], num_results)
                history[-1][1] = bot_message
            return history

        # Event handlers cho tab h·ªèi ƒë√°p
        msg.submit(user, [msg, chatbot], [msg, chatbot], queue=False).then(
            bot, [chatbot, num_results], chatbot
        )
        submit_btn.click(user, [msg, chatbot], [msg, chatbot], queue=False).then(
            bot, [chatbot, num_results], chatbot
        )
        clear_btn.click(lambda: None, None, chatbot, queue=False)


demo.launch(share=False, server_name="localhost", server_port=7860)
